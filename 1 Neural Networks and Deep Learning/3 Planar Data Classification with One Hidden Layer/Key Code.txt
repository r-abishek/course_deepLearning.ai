Logistic Regression vs single hidden layer Neural Network

# Train the logistic regression classifier
clf = sklearn.linear_model.LogisticRegressionCV();
clf.fit(X.T, Y.T);


### START CODE HERE ### (˜ 3 lines of code)
n_x = X.shape[0] # size of input layer
n_h = 4
n_y = Y.shape[0] # size of output layer
### END CODE HERE ###


### START CODE HERE ### (˜ 4 lines of code)
W1 = np.random.randn(n_h, n_x) * 0.01
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(n_y, n_h) * 0.01
b2 = np.zeros((n_y, 1))
### END CODE HERE ###


# Retrieve each parameter from the dictionary "parameters"
### START CODE HERE ### (˜ 4 lines of code)
W1 = parameters["W1"]
b1 = parameters["b1"]
W2 = parameters["W2"]
b2 = parameters["b2"]
### END CODE HERE ###
# Implement Forward Propagation to calculate A2 (probabilities)
### START CODE HERE ### (˜ 4 lines of code)
Z1 = np.dot(W1, X) + b1
A1 = np.tanh(Z1)
Z2 = np.dot(W2, A1) + b2
A2 = sigmoid(Z2)
### END CODE HERE ###

# Compute the cross-entropy cost
### START CODE HERE ### (˜ 2 lines of code)
logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))
cost = np.divide(np.sum(logprobs), -m)
### END CODE HERE ###


# First, retrieve W1 and W2 from the dictionary "parameters".
### START CODE HERE ### (˜ 2 lines of code)
W1 = parameters["W1"]
W2 = parameters["W2"]
### END CODE HERE ###
# Retrieve also A1 and A2 from dictionary "cache".
### START CODE HERE ### (˜ 2 lines of code)
A1 = cache["A1"]
A2 = cache["A2"]
### END CODE HERE ###
# Backward propagation: calculate dW1, db1, dW2, db2. 
### START CODE HERE ### (˜ 6 lines of code, corresponding to 6 equations on slide above)
dZ2 = A2 - Y
dW2 = np.divide(np.dot(dZ2, A1.T), m)
db2 = np.divide(np.sum(dZ2, axis = 1, keepdims = True), m)
dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
dW1 = np.divide(np.dot(dZ1, X.T), m)
db1 = np.divide(np.sum(dZ1, axis = 1, keepdims = True), m)
### END CODE HERE ###


